{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXdcjYklwpPn"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
    "2. Tokenize text by BPE with vocab_size = 100\n",
    "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
    "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
    "5. Calculate perplexity of the language model for the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2lsimsBwwpPp",
    "outputId": "771e734e-4970-477b-c536-9d355df4eac3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227579"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('peace.txt', 'r').read()[2:]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzyWtzLGwpPw"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # make lowercase\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    punct = punctuation.replace(\".\", \"\")\n",
    "    for p in punct:\n",
    "      text = text.replace(p, \"\")\n",
    "    text = text.replace(\". \", \",\")\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.replace(\",\", \". \") # replace all punctuation except dots with spaces\n",
    "    text = re.sub(r\" +\", \" \", text) # collapse multiple spaces into one '   ' -> ' '\n",
    "    return text\n",
    "\n",
    "text = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rvp-LYHwpP1"
   },
   "outputs": [],
   "source": [
    "text = text.split('.')\n",
    "text = [x.strip() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s00kUwhOwpP4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class BPE(TransformerMixin):\n",
    "    def __init__(self, vocab_size=100):\n",
    "        super(BPE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # index to token\n",
    "        self.itos = []\n",
    "        # token to index\n",
    "        self.stoi = {}\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        fit itos and stoi\n",
    "        text: list of strings \n",
    "        \"\"\"\n",
    "        symbols = set()\n",
    "\n",
    "        for sent in text:\n",
    "          for l in sent:\n",
    "            if l not in symbols:\n",
    "              symbols.add(l)\n",
    "              i = len(self.itos)\n",
    "              self.itos.append(l)\n",
    "              self.stoi[l] = i\n",
    "        \n",
    "        text = [[self.stoi[l] for l in sent] for sent in text]\n",
    "\n",
    "        while len(self.itos) < self.vocab_size:\n",
    "            new_list = []\n",
    "            for sent in text:\n",
    "              for i in range(len(sent) - 2):\n",
    "                new_list.append(sent[i:i+2])\n",
    "            c = Counter(map(tuple, new_list))\n",
    "\n",
    "            new_token = c.most_common(1)[0][0]\n",
    "            new_id = len(self.itos)\n",
    "            \n",
    "            self.itos.append(new_token)\n",
    "            \n",
    "            for i in range(len(text)):\n",
    "              sent = text[i]\n",
    "              for j in range(len(sent) - 2):\n",
    "                if sent[j:j+2] == list(new_token):\n",
    "                  sent[j] = new_id\n",
    "                  sent[j+1] = -1\n",
    "              text[i] = [l for l in sent if l >= 0]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        convert text to a sequence of token ids\n",
    "        text: list of strings\n",
    "        \"\"\"\n",
    "        text = [list(sent) for sent in text]\n",
    "\n",
    "        for token_id, token in enumerate(self.itos):\n",
    "            for i in range(len(text)):\n",
    "              sent = text[i]\n",
    "              if type(token) == str:\n",
    "                for j in range(len(sent)):\n",
    "                  if sent[j] == token:\n",
    "                    sent[j] = token_id\n",
    "              else:\n",
    "                for j in range(len(sent) - 2):\n",
    "                  if sent[j:j+2] == token:\n",
    "                    sent[j] = new_id\n",
    "                    sent[j+1] = -1\n",
    "                text[i] = [l for l in sent if l >= 0]\n",
    "        return text\n",
    "    \n",
    "    def decode_token(self, tok):\n",
    "        \"\"\"\n",
    "        tok: int or tuple\n",
    "        \"\"\"\n",
    "        if type(tok) == tuple:\n",
    "          return self.decode_token(tok[0]) + self.decode_token(tok[1])\n",
    "        else:\n",
    "          if type(self.itos[tok]) == list:\n",
    "            return(self.decode_token(tuple(tok)))\n",
    "          else:\n",
    "            return self.itos[tok]\n",
    "        return result\n",
    "            \n",
    "    def decode(self, text):\n",
    "        \"\"\"\n",
    "        convert token ids into text\n",
    "        \"\"\"\n",
    "        return ''.join(map(self.decode_token, text))\n",
    "        \n",
    "        \n",
    "vocab_size = 100\n",
    "bpe = BPE(vocab_size)\n",
    "tokenized_text = bpe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ww-mWExWwpP8"
   },
   "outputs": [],
   "source": [
    "assert bpe.decode(tokenized_text[0]) == text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SIuhGxBswpP-"
   },
   "outputs": [],
   "source": [
    "start_token = vocab_size\n",
    "end_token = vocab_size + 1\n",
    "\n",
    "\n",
    "class LM:\n",
    "    def __init__(self, vocab_size, delta=1):\n",
    "        self.delta = delta\n",
    "        self.vocab_size = vocab_size + 2\n",
    "        self.proba = {}\n",
    "        \n",
    "    def infer(self, a, b, tau=1):\n",
    "        \"\"\"\n",
    "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        result = {tok: self.get_proba(a, b, tok, tau)\n",
    "                  for tok in range(self.vocab_size) if tok != start_token}\n",
    "        return result\n",
    "        \n",
    "    def get_proba(self, a, b, c, tau=1):\n",
    "        \"\"\"\n",
    "        get probability of 3-gram (a,b,c)\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        c: third token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        proba2 = pow(self.proba[a][b][\"sum\"],\n",
    "                      1/tau) + (self.delta * self.vocab_size)\n",
    "        proba3 = pow(self.proba[a][b][c], 1/tau) + self.delta\n",
    "        result = proba3/proba2\n",
    "        return result\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        train language model on text\n",
    "        text: list of lists\n",
    "        \"\"\"\n",
    "        ttext = []\n",
    "        for sentence in text:\n",
    "          ttext += [start_token] + sentence + [end_token]\n",
    "        ttext = tuple(ttext)\n",
    "        grams = Counter([ttext[i:i+3] for i in range(len(ttext) - 2)])\n",
    "        for a in range(self.vocab_size):\n",
    "          if a == end_token:\n",
    "            continue\n",
    "          self.proba[a] = {}\n",
    "          for b in range(self.vocab_size):\n",
    "            if b in (start_token, end_token):\n",
    "              continue\n",
    "            self.proba[a][b] = {}\n",
    "            for c in range(self.vocab_size):\n",
    "              if c == start_token:\n",
    "                continue\n",
    "              self.proba[a][b][c] = grams[(a, b, c)]\n",
    "            self.proba[a][b][\"sum\"] = sum(self.proba[a][b].values())\n",
    "        return self\n",
    "    \n",
    "lm = LM(vocab_size, 1).fit(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aw8JzlAZwpQR",
    "outputId": "c82f6b1f-72f9-412d-fdf1-8e2f1e74f7e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.02930569801232"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity(snt, lm, tau=1):\n",
    "    \"\"\"\n",
    "    snt: sequence of token ids\n",
    "    lm: language model\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    for i in range(3, len(snt)+1):\n",
    "      result += lm.get_proba(snt[i-3], snt[i-2], snt[i-1], tau)\n",
    "    return result\n",
    "\n",
    "perplexity(tokenized_text[0], lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjXcZ1VbwpQC"
   },
   "outputs": [],
   "source": [
    "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
    "    \"\"\"\n",
    "    generate sequence from language model *lm* conditioned on input_seq\n",
    "    input_seq: sequence of token ids for conditioning\n",
    "    lm: language model\n",
    "    max_len: max generated sequence length\n",
    "    k: size of beam\n",
    "    tau: temperature\n",
    "    \"\"\"\n",
    "\n",
    "    input_seq = [start_token] + input_seq\n",
    "    max_len += 1\n",
    "    \n",
    "    candidates = [tuple(input_seq)] * k\n",
    "\n",
    "    for i in range(len(input_seq)-1, max_len):\n",
    "      beam = {}\n",
    "      for cand in candidates[:k]:\n",
    "        a = cand[-2]\n",
    "        b = cand[-1]\n",
    "        infer = lm.infer(a, b, tau)\n",
    "        beam.update({cand + (c,): infer[c] for c in infer})\n",
    "      topk = sorted(beam, key=beam.get, reverse=True)[:k]\n",
    "      candidates = [cand for cand in topk] + candidates[k:]\n",
    "      final_idx = [topk.index(c) for c in topk if c[-1] == end_token]\n",
    "      for idx in sorted(final_idx, reverse=True):\n",
    "        k -= 1\n",
    "        candidates[idx], candidates[k] = candidates[k], candidates[idx]\n",
    "    \n",
    "    for i, cand in enumerate(candidates):\n",
    "      if cand[-1] == end_token:\n",
    "        candidates[i] = cand[:-1]\n",
    "    \n",
    "    beam = {cand[1:]: perplexity(list(cand[1:]), lm, tau) for cand in candidates}\n",
    "    return beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "3ivzfhoUwpQG",
    "outputId": "f630cba1-9ba6-4192-e463-eed4a9f5292d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse of th: 0.3205284009362348\n",
      "horse whis : 0.010779854215407572\n",
      "horse whim : 7.417695060398774e-05\n",
      "horse of to: 0.30457968530688223\n",
      "horse ing t: 0.11830332061596541\n",
      "horse con t: 0.0007609758095371546\n",
      "horse and t: 0.07385003584715494\n",
      "horse whime: 5.163174084631868e-06\n",
      "horse whoun: 1.268587291925252e-05\n",
      "horse whout: 1.2671659078252562e-05\n"
     ]
    }
   ],
   "source": [
    "input1 = 'horse '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "for tok_seq in result:\n",
    "  print(bpe.decode(list(tok_seq))+\": \"+str(result[tok_seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "3ABbu7eSwpQJ",
    "outputId": "fc61139b-99b5-4a7e-fdce-00e8b9f42add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "herring to : 0.17375038397334014\n",
      "herring the: 0.14577341628206936\n",
      "hering the : 0.15736994688547143\n",
      "herrin the : 0.039457885444548015\n",
      "hervin the : 0.039428677879375505\n",
      "hering to t: 0.17374980333103449\n",
      "herrin to t: 0.055837741890111044\n",
      "hervin to t: 0.05580853432493854\n",
      "hering ther: 0.14577281864637437\n",
      "her: 1.17716386592672e-08\n"
     ]
    }
   ],
   "source": [
    "input1 = 'her'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "for tok_seq in result:\n",
    "  print(bpe.decode(list(tok_seq))+\": \"+str(result[tok_seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "sDCBz5NHwpQM",
    "outputId": "0e802324-a2f3-4d93-f7dc-8ec1c6572f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whatch and : 3.7562284192722237\n",
      "whation to : 3.4081621493779317\n",
      "whation the: 3.732546539319406\n",
      "whatch the : 3.930293395976595\n",
      "whatáshound: 3.4083774460125533\n",
      "whatáshere : 3.4707321807135987\n",
      "whatáshount: 3.2663582694756106\n",
      "whatáshad t: 3.568807885079943\n",
      "whatch to t: 3.1418967264841404\n",
      "whatáshat t: 3.70200399143037\n"
     ]
    }
   ],
   "source": [
    "input1 = 'what'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
    "for tok_seq in result:\n",
    "  print(bpe.decode(list(tok_seq))+\": \"+str(result[tok_seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "aVgx6TqVwpQP",
    "outputId": "69fb7af9-cb9b-4a29-b6b2-cc8e8cc81427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gun whound : 0.06811305022739671\n",
      "gun ing to : 0.17408737336268865\n",
      "gun ing the: 0.14611040567141784\n",
      "gun con the: 0.028568060864170935\n",
      "gun and the: 0.10165712542011845\n",
      "gun of the : 0.34398347760340886\n",
      "gun whimen : 1.2828408237037665e-05\n",
      "gun whiment: 7.583012182698783e-06\n",
      "gun of to t: 0.36036333404897186\n",
      "gun whout t: 0.01930485432917487\n"
     ]
    }
   ],
   "source": [
    "input1 = 'gun '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "for tok_seq in result:\n",
    "  print(bpe.decode(list(tok_seq))+\": \"+str(result[tok_seq]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
